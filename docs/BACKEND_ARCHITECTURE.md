# Veri-Train åç«¯æ¶æ„è®¾è®¡æ–‡æ¡£

> æœ¬æ–‡æ¡£ä¸ºåç«¯å¼€å‘å›¢é˜Ÿæä¾›å®Œæ•´çš„æ¶æ„è®¾è®¡å’Œå®ç°æŒ‡å—

## æŠ€æœ¯æ ˆ

| æŠ€æœ¯ | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| **FastAPI** | 0.100+ | Webæ¡†æ¶,æä¾›REST API |
| **Celery** | 5.x | å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—(è®­ç»ƒ/è¯„æµ‹/æ•°æ®ç”Ÿæˆ) |
| **PostgreSQL** | 15+ | ä¸»æ•°æ®åº“(å…ƒæ•°æ®å­˜å‚¨) |
| **Redis** | 7.x | Celery broker + ç¼“å­˜ |
| **SQLAlchemy** | 2.x | ORM |
| **Pydantic** | 2.x | æ•°æ®éªŒè¯ |
| **WebSocket** | - | å®æ—¶é€šä¿¡(FastAPIåŸç”Ÿ) |
| **Alembic** | - | æ•°æ®åº“è¿ç§» |

## ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Frontend (React)                      â”‚
â”‚              http://localhost:3000                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ HTTP/WebSocket
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Gateway                            â”‚
â”‚           http://localhost:8000                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  REST API Endpoints  â”‚  â”‚  WebSocket Server    â”‚    â”‚
â”‚  â”‚  /api/v1/*          â”‚  â”‚  /ws                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                            â”‚
       â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚            â”‚    Redis     â”‚
â”‚  Port: 5432  â”‚            â”‚  Port: 6379  â”‚
â”‚              â”‚            â”‚              â”‚
â”‚ - Models     â”‚            â”‚ - Task Queue â”‚
â”‚ - Datasets   â”‚            â”‚ - Cache      â”‚
â”‚ - Experimentsâ”‚            â”‚ - Sessions   â”‚
â”‚ - Evaluationsâ”‚            â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    Celery    â”‚
                            â”‚   Workers    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼              â–¼              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Training â”‚   â”‚Data Gen  â”‚   â”‚Evaluationâ”‚
              â”‚ Worker  â”‚   â”‚ Worker   â”‚   â”‚ Worker   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  GPU           GPT API      CPU/GPU
```

## é¡¹ç›®ç»“æ„

```
veri-train-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPIåº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py             # æ•°æ®åº“è¿æ¥
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ router.py       # è·¯ç”±èšåˆ
â”‚   â”‚       â”œâ”€â”€ deps.py         # ä¾èµ–æ³¨å…¥
â”‚   â”‚       â””â”€â”€ endpoints/
â”‚   â”‚           â”œâ”€â”€ models.py
â”‚   â”‚           â”œâ”€â”€ datasets.py
â”‚   â”‚           â”œâ”€â”€ experiments.py
â”‚   â”‚           â”œâ”€â”€ evaluation.py
â”‚   â”‚           â”œâ”€â”€ reports.py
â”‚   â”‚           â””â”€â”€ websocket.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # SQLAlchemyæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ experiment.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â””â”€â”€ report.py
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                # Pydanticæ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ experiment.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â””â”€â”€ common.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # ä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_service.py
â”‚   â”‚   â”œâ”€â”€ dataset_service.py
â”‚   â”‚   â”œâ”€â”€ experiment_service.py
â”‚   â”‚   â””â”€â”€ evaluation_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/                  # Celeryä»»åŠ¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â”œâ”€â”€ generation.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â””â”€â”€ quality_gate.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ security.py         # è®¤è¯æˆæƒ
â”‚   â”‚   â”œâ”€â”€ cache.py            # Redisç¼“å­˜
â”‚   â”‚   â””â”€â”€ websocket.py        # WebSocketç®¡ç†
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py          # è¯„æµ‹æŒ‡æ ‡è®¡ç®—
â”‚       â”œâ”€â”€ data_utils.py
â”‚       â””â”€â”€ model_utils.py
â”‚
â”œâ”€â”€ alembic/                    # æ•°æ®åº“è¿ç§»
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ env.py
â”‚
â”œâ”€â”€ tests/                      # æµ‹è¯•
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ tasks/
â”‚
â”œâ”€â”€ scripts/                    # è„šæœ¬
â”‚   â”œâ”€â”€ init_db.py
â”‚   â””â”€â”€ seed_data.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

## æ ¸å¿ƒAPIè®¾è®¡

### 1. Models API

```python
# app/api/v1/endpoints/models.py

@router.get("/models", response_model=PaginatedResponse[Model])
async def get_models(
    page: int = 1,
    page_size: int = 20,
    status: Optional[str] = None,
    type: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    ...

@router.get("/models/{model_id}", response_model=ModelDetail)
async def get_model_detail(
    model_id: str,
    db: Session = Depends(get_db)
):
    """è·å–æ¨¡å‹è¯¦æƒ…"""
    ...

@router.post("/models/{model_id}/probe", response_model=BaselineProbe)
async def run_baseline_probe(
    model_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """è¿è¡ŒåŸºçº¿è¡Œä¸ºæ¢æµ‹"""
    # è¿™æ˜¯å…³é”®åŠŸèƒ½!
    # æ£€æµ‹æ¨¡å‹æ˜¯å¦:
    # - æ”¯æŒå¤šå€™é€‰è¾“å‡º
    # - æä¾›è§£é‡Šæ€§è¾“å‡º
    # - éµå¾ªè¾“å‡ºå¥‘çº¦
    ...

@router.get("/models/{model_id}/evaluations")
async def get_model_evaluations(
    model_id: str,
    db: Session = Depends(get_db)
):
    """è·å–æ¨¡å‹è¯„æµ‹å†å²"""
    ...
```

### 2. Datasets API

```python
# app/api/v1/endpoints/datasets.py

@router.post("/datasets", response_model=Dataset)
async def upload_dataset(
    file: UploadFile,
    metadata: str = Form(...),
    db: Session = Depends(get_db)
):
    """ä¸Šä¼ æ•°æ®é›†"""
    # 1. ä¿å­˜æ–‡ä»¶
    # 2. è§£æå…ƒæ•°æ®
    # 3. è§¦å‘Quality Gateæ£€æŸ¥(Celeryä»»åŠ¡)
    ...

@router.post("/datasets/generate/estimate", response_model=GenerateEstimate)
async def estimate_generation_cost(
    config: GenerateDatasetConfig,
    db: Session = Depends(get_db)
):
    """ä¼°ç®—æ•°æ®ç”Ÿæˆæˆæœ¬"""
    # æ ¹æ®é…ç½®è®¡ç®—:
    # - Tokenæ•°é‡
    # - GPT APIæˆæœ¬
    # - é¢„ä¼°æ—¶é—´
    ...

@router.post("/datasets/generate", response_model=TaskResponse)
async def generate_dataset(
    config: GenerateDatasetConfig,
    db: Session = Depends(get_db),
    background_tasks: BackgroundTasks
):
    """ç”Ÿæˆæ•°æ®é›†(å¼‚æ­¥)"""
    # å¯åŠ¨Celeryä»»åŠ¡
    task = tasks.generate_dataset.delay(config.dict())
    return {"task_id": task.id}

@router.get("/datasets/{dataset_id}/quality-gate", response_model=QualityGateResult)
async def get_quality_gate(
    dataset_id: str,
    db: Session = Depends(get_db)
):
    """è·å–è´¨é‡é—¨ç¦ç»“æœ"""
    # è¿”å›:
    # - å¯¹é½ç‡
    # - é‡å¤ç‡
    # - è¯­è¨€ä¸€è‡´æ€§
    # - æŠ½æ ·äººå·¥è¯„å®¡
    ...
```

### 3. Experiments API

```python
# app/api/v1/endpoints/experiments.py

@router.post("/experiments", response_model=Experiment)
async def create_experiment(
    experiment: ExperimentCreate,
    db: Session = Depends(get_db)
):
    """åˆ›å»ºå®éªŒ"""
    # éªŒè¯:
    # - Datasetæ˜¯å¦passedè´¨é‡é—¨ç¦
    # - Modelæ˜¯å¦available
    # - Configæ˜¯å¦åˆæ³•
    ...

@router.post("/experiments/{experiment_id}/start", response_model=Experiment)
async def start_experiment(
    experiment_id: str,
    db: Session = Depends(get_db)
):
    """å¯åŠ¨å®éªŒ"""
    # 1. æ›´æ–°çŠ¶æ€ä¸ºrunning
    # 2. è§¦å‘Celeryè®­ç»ƒä»»åŠ¡
    # 3. è¿”å›task_id
    task = tasks.train_model.delay(experiment_id)
    ...

@router.get("/experiments/{experiment_id}/logs")
async def get_experiment_logs(
    experiment_id: str,
    limit: int = 100,
    offset: int = 0,
    db: Session = Depends(get_db)
):
    """è·å–å®éªŒæ—¥å¿—"""
    ...

# WebSocketç«¯ç‚¹
@router.websocket("/experiments/{experiment_id}/stream")
async def experiment_stream(
    websocket: WebSocket,
    experiment_id: str
):
    """å®éªŒå®æ—¶è¿›åº¦æµ"""
    await websocket.accept()

    # è®¢é˜…Redis Pub/Sub
    # Workerä¼šå‘å¸ƒè¿›åº¦åˆ°Redis
    # è¿™é‡Œä»Redisè¯»å–å¹¶æ¨é€ç»™å‰ç«¯
    async for message in redis_pubsub.listen():
        await websocket.send_json(message)
```

### 4. WebSocket API

```python
# app/api/v1/endpoints/websocket.py

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""

    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}

    async def connect(self, client_id: str, websocket: WebSocket):
        await websocket.accept()
        if client_id not in self.active_connections:
            self.active_connections[client_id] = []
        self.active_connections[client_id].append(websocket)

    async def disconnect(self, client_id: str, websocket: WebSocket):
        self.active_connections[client_id].remove(websocket)

    async def broadcast_to_client(self, client_id: str, message: dict):
        if client_id in self.active_connections:
            for connection in self.active_connections[client_id]:
                await connection.send_json(message)

manager = ConnectionManager()

@router.websocket("/ws")
async def websocket_endpoint(
    websocket: WebSocket,
    token: str = Query(...)
):
    """WebSocketä¸»ç«¯ç‚¹"""
    # 1. éªŒè¯token
    user = await authenticate_ws_token(token)

    # 2. å»ºç«‹è¿æ¥
    await manager.connect(user.id, websocket)

    try:
        # 3. ç›‘å¬å®¢æˆ·ç«¯è®¢é˜…è¯·æ±‚
        async for message in websocket.iter_json():
            if message["type"] == "subscribe":
                # è®¢é˜…ç‰¹å®šå®éªŒ/æ•°æ®é›†
                await handle_subscription(user.id, message)
    except WebSocketDisconnect:
        manager.disconnect(user.id, websocket)
```

## Celeryä»»åŠ¡è®¾è®¡

### 1. è®­ç»ƒä»»åŠ¡

```python
# app/tasks/training.py

@celery_app.task(bind=True)
def train_model(self, experiment_id: str):
    """è®­ç»ƒæ¨¡å‹ä»»åŠ¡"""

    # 1. åŠ è½½å®éªŒé…ç½®
    experiment = db.query(Experiment).get(experiment_id)

    # 2. å‡†å¤‡æ•°æ®
    dataset = prepare_dataset(experiment.dataset_id)

    # 3. åŠ è½½æ¨¡å‹
    model = load_model(experiment.base_model_id)

    # 4. è®­ç»ƒå¾ªç¯
    for epoch in range(experiment.config.epochs):
        for step, batch in enumerate(dataloader):
            # è®­ç»ƒæ­¥éª¤
            loss = train_step(model, batch)

            # å‘å¸ƒè¿›åº¦åˆ°Redis
            progress = {
                "experiment_id": experiment_id,
                "epoch": epoch,
                "step": step,
                "loss": float(loss),
                "gpu_util": get_gpu_utilization(),
                "eta": calculate_eta()
            }
            redis_client.publish(
                f"experiment:{experiment_id}",
                json.dumps(progress)
            )

            # æ›´æ–°Celeryä»»åŠ¡çŠ¶æ€
            self.update_state(
                state='PROGRESS',
                meta=progress
            )

    # 5. ä¿å­˜æ¨¡å‹
    save_checkpoint(model, experiment_id)

    # 6. è§¦å‘è¯„æµ‹
    evaluate_model.delay(experiment_id)

    return {"status": "completed"}
```

### 2. æ•°æ®ç”Ÿæˆä»»åŠ¡

```python
# app/tasks/generation.py

@celery_app.task(bind=True)
def generate_dataset(self, config: dict):
    """ç”Ÿæˆæ•°æ®é›†ä»»åŠ¡"""

    # 1. åŠ è½½seedæ•°æ®
    seed_data = load_seed(config["seed_source"])

    # 2. è°ƒç”¨GPT APIç”Ÿæˆ
    generated_samples = []

    for i, seed in enumerate(seed_data):
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆprompt
        prompt = build_generation_prompt(seed, config["strategy"])

        # è°ƒç”¨GPT
        response = openai.ChatCompletion.create(
            model=config["strategy"]["model"],
            messages=[{"role": "user", "content": prompt}]
        )

        generated_samples.append(response.choices[0].message.content)

        # æ›´æ–°è¿›åº¦
        progress = (i + 1) / len(seed_data) * 100
        self.update_state(
            state='PROGRESS',
            meta={"progress": progress}
        )

    # 3. ä¿å­˜æ•°æ®é›†
    dataset_id = save_generated_dataset(generated_samples, config)

    # 4. è§¦å‘è´¨é‡é—¨ç¦
    check_quality_gate.delay(dataset_id)

    return {"dataset_id": dataset_id}
```

### 3. è´¨é‡é—¨ç¦ä»»åŠ¡

```python
# app/tasks/quality_gate.py

@celery_app.task
def check_quality_gate(dataset_id: str):
    """è´¨é‡é—¨ç¦æ£€æŸ¥ä»»åŠ¡"""

    dataset = db.query(Dataset).get(dataset_id)
    data = load_dataset_content(dataset)

    # 1. å¯¹é½ç‡æ£€æŸ¥
    alignment_rate = calculate_alignment_rate(data)

    # 2. é‡å¤ç‡æ£€æŸ¥
    duplicate_rate = calculate_duplicate_rate(data)

    # 3. è¯­è¨€ä¸€è‡´æ€§æ£€æŸ¥
    language_consistency = check_language_consistency(data)

    # 4. æŠ½æ ·GPTè¯„åˆ†
    sample_scores = []
    for sample in random.sample(data, min(100, len(data))):
        score = gpt_evaluate_quality(sample)
        sample_scores.append(score)

    # 5. åˆ¤å®š
    result = QualityGateResult(
        alignment_rate=alignment_rate,
        duplicate_rate=duplicate_rate,
        language_consistency=language_consistency,
        avg_sample_score=np.mean(sample_scores)
    )

    # é˜ˆå€¼åˆ¤å®š
    if (result.alignment_rate < 0.8 or
        result.duplicate_rate > 0.2 or
        result.language_consistency < 0.9):
        dataset.status = "blocked"
        result.status = "failed"
    else:
        dataset.status = "passed"
        result.status = "passed"

    # 6. ä¿å­˜ç»“æœ
    dataset.quality_gate_result = result.dict()
    db.commit()

    # 7. é€šçŸ¥å‰ç«¯
    redis_client.publish(
        f"dataset:{dataset_id}",
        json.dumps({
            "type": "quality_gate",
            "result": result.dict()
        })
    )

    return result.dict()
```

## æ•°æ®åº“æ¨¡å‹è®¾è®¡

```python
# app/models/experiment.py

class Experiment(Base):
    __tablename__ = "experiments"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)

    # å…³è”
    dataset_id = Column(UUID(as_uuid=True), ForeignKey("datasets.id"))
    base_model_id = Column(UUID(as_uuid=True), ForeignKey("models.id"))
    adapter_id = Column(UUID(as_uuid=True), ForeignKey("models.id"), nullable=True)
    prompt_contract_id = Column(UUID(as_uuid=True), ForeignKey("prompt_contracts.id"))

    # é…ç½®(JSONB)
    config = Column(JSONB, nullable=False)

    # çŠ¶æ€
    status = Column(String(50), default="pending")

    # ç»“æœ
    metrics = Column(JSONB)
    best_checkpoint_path = Column(String(500))

    # æ—¶é—´æˆ³
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)

    # å…³ç³»
    dataset = relationship("Dataset", back_populates="experiments")
    base_model = relationship("Model", foreign_keys=[base_model_id])
    evaluations = relationship("Evaluation", back_populates="experiment")
```

## é…ç½®ç®¡ç†

```python
# app/config.py

from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # APIé…ç½®
    API_V1_PREFIX: str = "/api/v1"
    PROJECT_NAME: str = "Veri-Train API"

    # æ•°æ®åº“
    DATABASE_URL: str

    # Redis
    REDIS_URL: str = "redis://localhost:6379/0"

    # Celery
    CELERY_BROKER_URL: str = "redis://localhost:6379/1"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/2"

    # Azure OpenAI
    AZURE_OPENAI_KEY: str
    AZURE_OPENAI_ENDPOINT: str

    # è®¤è¯
    SECRET_KEY: str
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

    # æ–‡ä»¶å­˜å‚¨
    UPLOAD_DIR: str = "./uploads"
    CHECKPOINT_DIR: str = "./checkpoints"

    class Config:
        env_file = ".env"

settings = Settings()
```

## Dockeréƒ¨ç½²

```yaml
# docker-compose.yml

version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: veritrain
      POSTGRES_USER: veritrain
      POSTGRES_PASSWORD: veritrain
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  api:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    environment:
      - DATABASE_URL=postgresql://veritrain:veritrain@postgres/veritrain
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./app:/app/app
      - ./uploads:/app/uploads
      - ./checkpoints:/app/checkpoints

  celery_worker:
    build: .
    command: celery -A app.tasks.celery_app worker --loglevel=info
    depends_on:
      - postgres
      - redis
    environment:
      - DATABASE_URL=postgresql://veritrain:veritrain@postgres/veritrain
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./app:/app/app
      - ./checkpoints:/app/checkpoints

  celery_beat:
    build: .
    command: celery -A app.tasks.celery_app beat --loglevel=info
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0

volumes:
  postgres_data:
```

## å¼€å‘å¯åŠ¨æµç¨‹

```bash
# 1. å¯åŠ¨æ•°æ®åº“å’ŒRedis
docker-compose up -d postgres redis

# 2. åˆ›å»ºæ•°æ®åº“è¡¨
alembic upgrade head

# 3. å¯åŠ¨FastAPI
uvicorn app.main:app --reload

# 4. å¯åŠ¨Celery Worker
celery -A app.tasks.celery_app worker --loglevel=info

# 5. (å¯é€‰)å¯åŠ¨Flowerç›‘æ§
celery -A app.tasks.celery_app flower
```

## å…³é”®å®ç°è¦ç‚¹

### 1. Baseline Probeå®ç°

è¿™æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒåˆ›æ–°åŠŸèƒ½,ç”¨äºæ£€æµ‹æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›:

```python
def run_baseline_probe(model_id: str) -> BaselineProbe:
    """
    æ£€æµ‹æ¨¡å‹çš„åŸºçº¿è¡Œä¸º:
    1. æ˜¯å¦æ”¯æŒå¤šå€™é€‰è¾“å‡º(n>1)
    2. æ˜¯å¦æä¾›è§£é‡Šæ€§è¾“å‡º
    3. æ˜¯å¦éµå¾ªè¾“å‡ºå¥‘çº¦(æ ¼å¼çº¦æŸ)
    """

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "input": "ä¼šè­°ã¯æ˜æ—¥é–‹å‚¬ã•ã‚Œã¾ã™ã€‚",
            "expected_behavior": "multi_candidate"
        },
        # ...
    ]

    results = {
        "is_multi_candidate": False,
        "has_explanation": False,
        "follows_output_contract": True
    }

    for case in test_cases:
        # è°ƒç”¨æ¨¡å‹
        output = call_model(model_id, case["input"], n=3)

        # åˆ†æç»“æœ
        if len(output.candidates) > 1:
            results["is_multi_candidate"] = True

        if "explanation" in output:
            results["has_explanation"] = True

        # ...

    return BaselineProbe(**results)
```

### 2. Quality Gateå®ç°

```python
QUALITY_GATE_THRESHOLDS = {
    "alignment_rate": 0.8,      # å¯¹é½ç‡â‰¥80%
    "duplicate_rate": 0.2,      # é‡å¤ç‡â‰¤20%
    "language_consistency": 0.9 # è¯­è¨€ä¸€è‡´æ€§â‰¥90%
}

def check_quality_gate(dataset: Dataset) -> QualityGateResult:
    """
    æ•°æ®è´¨é‡é—¨ç¦æ£€æŸ¥
    """
    # è®¡ç®—æŒ‡æ ‡...

    # åˆ¤å®š
    passed = all([
        metrics["alignment_rate"] >= QUALITY_GATE_THRESHOLDS["alignment_rate"],
        metrics["duplicate_rate"] <= QUALITY_GATE_THRESHOLDS["duplicate_rate"],
        metrics["language_consistency"] >= QUALITY_GATE_THRESHOLDS["language_consistency"]
    ])

    return QualityGateResult(
        status="passed" if passed else "failed",
        metrics=metrics
    )
```

## ç›‘æ§å’Œæ—¥å¿—

```python
# ä½¿ç”¨structlogè¿›è¡Œç»“æ„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger()

@router.post("/experiments/{experiment_id}/start")
async def start_experiment(experiment_id: str):
    logger.info(
        "experiment_started",
        experiment_id=experiment_id,
        user_id=current_user.id
    )
    # ...
```

## å®‰å…¨æ€§

1. **JWTè®¤è¯**: æ‰€æœ‰APIéƒ½éœ€è¦Bearer Token
2. **CORSé…ç½®**: é™åˆ¶å…è®¸çš„å‰ç«¯åŸŸå
3. **SQLæ³¨å…¥é˜²æŠ¤**: ä½¿ç”¨ORMå‚æ•°åŒ–æŸ¥è¯¢
4. **æ–‡ä»¶ä¸Šä¼ éªŒè¯**: é™åˆ¶æ–‡ä»¶ç±»å‹å’Œå¤§å°
5. **Rate Limiting**: ä½¿ç”¨slowapié™åˆ¶è¯·æ±‚é¢‘ç‡

---

**çŠ¶æ€**: ğŸ“‹ æ¶æ„è®¾è®¡å®Œæˆ,ç­‰å¾…å®ç°

**ä¼˜å…ˆçº§**:
1. FastAPIåŸºç¡€æ¡†æ¶ + æ•°æ®åº“æ¨¡å‹
2. Models/Datasets API
3. Celeryè®­ç»ƒä»»åŠ¡
4. WebSocketå®æ—¶é€šä¿¡
5. Quality Gateå’ŒBaseline Probe
